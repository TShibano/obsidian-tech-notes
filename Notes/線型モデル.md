---
title: "線型モデル"
date: 2026-02-13
tags:
  - AI
  - ML
related:
  - "[[MLOps]]"
  - "[[ベイズ統計]]"
  - "[[CatBoost]]"
---

## 概要

線型モデル（線形モデル）は、目的変数を説明変数の線形結合として表現する最も基本的な統計・機械学習モデルの総称。線形回帰・ロジスティック回帰・一般化線形モデル（GLM）などがあり、解釈可能性が高くデータが少ない場合でも有効。正則化（Ridge・Lasso・Elastic Net）や一般化（GLM・GAM）による豊富な拡張手法を持つ。

## 詳細

### 基本: 線形回帰

目的変数 $y$ を説明変数 $\mathbf{x}$ の線形結合で予測する：

$$\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p$$

**仮定:**
- 誤差項の正規性・等分散性・独立性
- 説明変数間の多重共線性がない

**最小二乗法（OLS）:**
残差二乗和 $\sum (y_i - \hat{y}_i)^2$ を最小化してパラメータを推定する。

### 拡張手法 1: 正則化回帰

過学習対策・特徴量選択のために正則化項をコスト関数に加える：

| 手法 | 正則化項 | 特徴 |
|------|---------|------|
| **Ridge回帰（L2）** | $\lambda \sum \beta_j^2$ | 係数を縮小するが0にはしない。多重共線性に強い |
| **Lasso回帰（L1）** | $\lambda \sum |\beta_j|$ | 不要な変数の係数を完全に0にする（スパースモデル）。変数選択に使える |
| **Elastic Net** | $\lambda_1 \sum |\beta_j| + \lambda_2 \sum \beta_j^2$ | L1とL2の混合。相関した変数グループを保持しつつスパース化 |

**使い分け:**
- 変数が多く選択したい → **Lasso**
- 相関変数が多い → **Ridge** or **Elastic Net**
- 両方のバランスが不明 → **Elastic Net**

```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler

# 特徴量の標準化（正則化前に必須）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

# Ridge
ridge = Ridge(alpha=1.0)
ridge.fit(X_scaled, y_train)

# Lasso
lasso = Lasso(alpha=0.1)
lasso.fit(X_scaled, y_train)

# Elastic Net
enet = ElasticNet(alpha=0.1, l1_ratio=0.5)
enet.fit(X_scaled, y_train)
```

### 拡張手法 2: 一般化線形モデル（GLM）

通常の線形回帰は「目的変数が正規分布に従う」仮定を持つ。GLMはこの仮定を緩め、任意の指数型分布族（正規・二項・ポアソン等）に対応する：

$$g(\mu) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p$$

ここで $g(\cdot)$ は**リンク関数**、$\mu = E[y]$。

| モデル | 分布 | リンク関数 | 用途 |
|--------|-----|-----------|------|
| 線形回帰 | 正規分布 | 恒等関数 | 連続値予測 |
| **ロジスティック回帰** | 二項分布 | ロジット関数 | 2値分類 |
| **ポアソン回帰** | ポアソン分布 | 対数関数 | カウントデータ |
| **負の二項回帰** | 負の二項分布 | 対数関数 | 過分散カウントデータ |
| 多項ロジスティック回帰 | 多項分布 | ソフトマックス | 多値分類 |

```python
from sklearn.linear_model import LogisticRegression
import statsmodels.api as sm

# scikit-learn でのロジスティック回帰
log_reg = LogisticRegression(C=1.0, solver='lbfgs')
log_reg.fit(X_train, y_train)

# statsmodels でのポアソン回帰（GLM）
glm_model = sm.GLM(y, X, family=sm.families.Poisson())
result = glm_model.fit()
print(result.summary())
```

### 拡張手法 3: 一般化加法モデル（GAM）

GLMの線形項 $\beta x$ を非線形なスムーズ関数 $f(x)$ に置き換え、非線形関係を扱えるようにしたモデル：

$$g(\mu) = f_1(x_1) + f_2(x_2) + \cdots + f_p(x_p)$$

解釈可能性を保ちながら非線形パターンを捉えられる。`pygam`ライブラリで実装可能。

### どのモデルを選ぶか

```
データが少ない・解釈が必要？
  ↓ Yes
  線形モデル系を検討
    カテゴリカル変数が多い → ロジスティック回帰 / GLM
    非線形関係がありそう → GAM
    変数が多い・過学習が心配 → Ridge / Lasso / Elastic Net
  ↓ No
  ツリー系（CatBoost, XGBoost等）または深層学習を検討
```

## ポイント

- 線型モデルは「解釈可能性」「データ効率」「計算効率」に優れ、ベースラインモデルとして必須
- 正則化（Ridge・Lasso）は変数スケーリング（標準化）が前提なので忘れずに
- ロジスティック回帰は分類問題において解釈しやすい確率モデルとして今も現役
- GLMはデータの分布に応じてリンク関数・分布族を選ぶことが重要
- [[ベイズ統計]]的アプローチと組み合わせることで、事前知識を取り込んだベイズ線形回帰（Ridge=L2事前分布、Lasso=Laplace事前分布に対応）も可能

## 関連項目

- [[ベイズ統計]]
- [[CatBoost]]
- [[MLOps]]

## 参考

- [Linear Regression - Google for Developers](https://developers.google.com/machine-learning/crash-course/linear-regression)
- [一般化線形モデル - Wikipedia](https://ja.wikipedia.org/wiki/%E4%B8%80%E8%88%AC%E5%8C%96%E7%B7%9A%E5%BD%A2%E3%83%A2%E3%83%87%E3%83%AB)
- [Regularization - scikit-learn Documentation](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)
- [What is Linear Regression? - AWS](https://aws.amazon.com/what-is/linear-regression/)
